"""
Author: Ahmed Elhelow                           1823229
Help was given by Ibis Prevedello and Michele Cipriano
"""

from __future__ import division
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import confusion_matrix
import csv
import os, os.path
import random
import time

folder = 'drebin/feature_vectors/'

# Set seed for random numbers
random.seed(5)

# List of all the files
dataset = os.listdir(folder)
print 'done reading all files'

# Create malware list with each file name
with open(folder + '../sha256_family.csv') as csvfile:
    reader = csv.reader(csvfile)
    next(reader)
    malware = [row[0] for row in reader]
print 'done reading csv file'

# Generate random numbers to get a random set of non-malware files
index = random.sample(range(0, len(dataset)-len(malware)-1), len(malware))
non_malware = [list(set(dataset)-set(malware))[i] for i in index]
print 'done creating the non-malware list'

# Merged list containing malware and non malware
data = malware + non_malware
random.shuffle(data)

# Print number of examples in each class
print('Number of non malware: ', len(non_malware))
print('Number of malware: ', len(malware))

##==========================================================##

# assign each feature to True or False
features = {
    'api_call': True,
    'permission': True,
    'intent': True,
    'call': True,
    'real_permission': True,
    'provider': True,
    'service_receiver': True,
    'activity': False,
    'feature': False,
    'url': False
}

# Creating a vocabulary from the dataset:
vocabulary = []
X = []
Y = []
cnt = 0
percent = 0

for file in data:
    cnt = cnt + 1
    if (1.0 * cnt / len(data)) - percent >= 0.1:
        percent = 1.0 * cnt / len(data)
        print 'Progress: {:2.1%}'.format(percent)
    try:
        features_vector = [0] * len(vocabulary)
        for feature in open(folder + file):
            if features[feature.split("::")[0]] == False:
                continue
            feature = feature.rstrip()
            if feature in vocabulary:
                features_vector[vocabulary.index(feature)] = 1
            else:
                vocabulary.append(feature)
                features_vector.append(1)
        X.append(features_vector)
        Y.append(1 if file in malware else 0)
    except Exception as e:
        pass

print 'done constructing X, Y initial values'

# fix X to have the same length as vocabulary
for i in range(len(X)):
    X[i] = X[i] + [0]*(len(vocabulary)-len(X[i]))

print 'done fixing X to have the same length as vocabulary'

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

print('Training set size:', len(X_train))
print('Test set size:', len(X_test))

##==========================================================##

# Implement Naive Bayes
def train_naive_bayes_classifer(X_train, X_test, Y_train):
    # Define the classifier
    gnb = BernoulliNB(alpha=1.0, binarize=None)
    
    # Check the training time
    t=time.time()
    gnb.fit(X_train, Y_train)
    t2 = time.time()
    print(round(t2-t, 2), 'Seconds to train Naive Bayes...')
    
    # Check the score of the SVC
    Y_predict=gnb.predict(X_test)

    return Y_predict

# Implement Support Vector Machine
def train_svm_classifer(X_train, X_test, Y_train):
    # Define the classifier
    svc = svm.LinearSVC(C=1.0, max_iter=4000) 
    
    # Check the training time
    t=time.time()
    svc.fit(X_train, Y_train)
    t2 = time.time()
    print(round(t2-t, 2), 'Seconds to train SVM...')
    
    # Check the score of the SVC
    Y_predict=svc.predict(X_test)

    return Y_predict

##==========================================================##

# Train SVM Classifier
print "Train SVM Classifier..."
Y_predict = train_svm_classifer(X_train, X_test, Y_train)

# Get metrics True Negative, False Positive,
# False Negative, True Positive
tn, fp, fn, tp = confusion_matrix(Y_test, Y_predict).\
ravel()

print 'True Negative: ', tn
print 'False Positive: ', fp
print 'False Negative: ', fn
print 'True Positive: ', tp

# Accuracy
accuracy = (tp + tn)/(fp + fn + tp + tn)
print 'accuracy: ' + str(round(accuracy*100,2)) + '%'

# Precision
precision = tp/(tp + fp)
print 'precision: ' + str(round(precision*100,2)) + '%'

# Recall
recall = tp/(tp + fn)
print 'recall: ' + str(round(recall*100,2)) + '%'

# False Positive Rate
false_pos_rate = fp/(fp + tn)
print 'false_pos_rate: ' + str(round(false_pos_rate*100,2)) + '%'

# F-Measure
f_measure = 2 *(precision * recall)/(precision + recall)
print 'f_measure: ' + str(round(f_measure*100,2)) + '%'

##==========================================================##

# Train Naive Bayes Classifier
print 'Train Naive Bayes Classifier...'
Y_predict = train_naive_bayes_classifer(X_train,\
X_test, Y_train)

# Get metrics True Negative, False Positive,
# False Negative, True Positive
tn, fp, fn, tp = confusion_matrix(Y_test, Y_predict).\
ravel()

print 'True Negative: ', tn
print 'False Positive: ', fp
print 'False Negative: ', fn
print 'True Positive: ', tp

# Accuracy
accuracy = (tp + tn)/(fp + fn + tp + tn)
print 'accuracy: ' + str(round(accuracy*100,2)) + '%'

# Precision
precision = tp/(tp + fp)
print 'precision: ' + str(round(precision*100,2)) + '%'

# Recall
recall = tp/(tp + fn)
print 'recall: ' + str(round(recall*100,2)) + '%'

# False Positive Rate
false_pos_rate = fp/(fp + tn)
print 'false_pos_rate: ' + str(round(false_pos_rate*100,2)) + '%'

# F-Measure
f_measure = 2 *(precision * recall)/(precision + recall)
print 'f_measure: ' + str(round(f_measure*100,2)) + '%'
##==========================================================##
